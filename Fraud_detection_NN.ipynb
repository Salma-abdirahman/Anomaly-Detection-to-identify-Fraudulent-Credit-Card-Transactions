{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "JkaOU29NklV1"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "import sklearn as skl\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"creditcardfraud_normalised.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "GdagcrVik72N",
        "outputId": "4cf18de5-3f28-46d0-b46a-90cdecb7e9a2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0       0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815   \n",
              "1       0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
              "2       0.935217  0.753118  0.868141  0.268766  0.762329  0.281122  0.270177   \n",
              "3       0.941878  0.765304  0.868484  0.213661  0.765647  0.275559  0.266803   \n",
              "4       0.938617  0.776520  0.864251  0.269796  0.762975  0.263984  0.268968   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "284802  0.756448  0.873531  0.666991  0.160317  0.729603  0.236810  0.235393   \n",
              "284803  0.945845  0.766677  0.872678  0.219189  0.771561  0.273661  0.265504   \n",
              "284804  0.990905  0.764080  0.781102  0.227202  0.783425  0.293496  0.263547   \n",
              "284805  0.954209  0.772856  0.849587  0.282508  0.763172  0.269291  0.261175   \n",
              "284806  0.949232  0.765256  0.849601  0.229488  0.765632  0.256488  0.274963   \n",
              "\n",
              "              V8        V9       V10  ...       V21       V22       V23  \\\n",
              "0       0.786444  0.475312  0.510600  ...  0.561184  0.522992  0.663793   \n",
              "1       0.786298  0.453981  0.505267  ...  0.557840  0.480237  0.666938   \n",
              "2       0.788042  0.410603  0.513018  ...  0.565477  0.546030  0.678939   \n",
              "3       0.789434  0.414999  0.507585  ...  0.559734  0.510277  0.662607   \n",
              "4       0.782484  0.490950  0.524303  ...  0.561327  0.547271  0.663392   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "284802  0.863749  0.528729  0.598850  ...  0.564920  0.515249  0.680500   \n",
              "284803  0.788548  0.482925  0.488530  ...  0.564933  0.553154  0.665619   \n",
              "284804  0.792985  0.477677  0.498692  ...  0.565220  0.537005  0.664877   \n",
              "284805  0.792671  0.476287  0.500464  ...  0.565755  0.547353  0.663008   \n",
              "284806  0.780938  0.479528  0.489782  ...  0.565688  0.540031  0.671029   \n",
              "\n",
              "             V24       V25       V26       V27       V28    Amount  class  \n",
              "0       0.391253  0.585122  0.394557  0.418976  0.312697  0.005824      0  \n",
              "1       0.336440  0.587290  0.446013  0.416345  0.313423  0.000105      0  \n",
              "2       0.289354  0.559515  0.402727  0.415489  0.311911  0.014739      0  \n",
              "3       0.223826  0.614245  0.389197  0.417669  0.314371  0.004807      0  \n",
              "4       0.401270  0.566343  0.507497  0.420561  0.317490  0.002724      0  \n",
              "...          ...       ...       ...       ...       ...       ...    ...  \n",
              "284802  0.313600  0.658558  0.466291  0.433929  0.329840  0.000030      0  \n",
              "284803  0.245298  0.543855  0.360884  0.417775  0.312038  0.000965      0  \n",
              "284804  0.468492  0.592823  0.411176  0.416593  0.312585  0.002642      0  \n",
              "284805  0.398836  0.545958  0.514746  0.418520  0.315245  0.000389      0  \n",
              "284806  0.383420  0.551319  0.291786  0.416466  0.313401  0.008446      0  \n",
              "\n",
              "[284807 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b3cf9a7-0d1b-49f5-ae14-ba59aa4d6f06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.935192</td>\n",
              "      <td>0.766490</td>\n",
              "      <td>0.881365</td>\n",
              "      <td>0.313023</td>\n",
              "      <td>0.763439</td>\n",
              "      <td>0.267669</td>\n",
              "      <td>0.266815</td>\n",
              "      <td>0.786444</td>\n",
              "      <td>0.475312</td>\n",
              "      <td>0.510600</td>\n",
              "      <td>...</td>\n",
              "      <td>0.561184</td>\n",
              "      <td>0.522992</td>\n",
              "      <td>0.663793</td>\n",
              "      <td>0.391253</td>\n",
              "      <td>0.585122</td>\n",
              "      <td>0.394557</td>\n",
              "      <td>0.418976</td>\n",
              "      <td>0.312697</td>\n",
              "      <td>0.005824</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.978542</td>\n",
              "      <td>0.770067</td>\n",
              "      <td>0.840298</td>\n",
              "      <td>0.271796</td>\n",
              "      <td>0.766120</td>\n",
              "      <td>0.262192</td>\n",
              "      <td>0.264875</td>\n",
              "      <td>0.786298</td>\n",
              "      <td>0.453981</td>\n",
              "      <td>0.505267</td>\n",
              "      <td>...</td>\n",
              "      <td>0.557840</td>\n",
              "      <td>0.480237</td>\n",
              "      <td>0.666938</td>\n",
              "      <td>0.336440</td>\n",
              "      <td>0.587290</td>\n",
              "      <td>0.446013</td>\n",
              "      <td>0.416345</td>\n",
              "      <td>0.313423</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.935217</td>\n",
              "      <td>0.753118</td>\n",
              "      <td>0.868141</td>\n",
              "      <td>0.268766</td>\n",
              "      <td>0.762329</td>\n",
              "      <td>0.281122</td>\n",
              "      <td>0.270177</td>\n",
              "      <td>0.788042</td>\n",
              "      <td>0.410603</td>\n",
              "      <td>0.513018</td>\n",
              "      <td>...</td>\n",
              "      <td>0.565477</td>\n",
              "      <td>0.546030</td>\n",
              "      <td>0.678939</td>\n",
              "      <td>0.289354</td>\n",
              "      <td>0.559515</td>\n",
              "      <td>0.402727</td>\n",
              "      <td>0.415489</td>\n",
              "      <td>0.311911</td>\n",
              "      <td>0.014739</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.941878</td>\n",
              "      <td>0.765304</td>\n",
              "      <td>0.868484</td>\n",
              "      <td>0.213661</td>\n",
              "      <td>0.765647</td>\n",
              "      <td>0.275559</td>\n",
              "      <td>0.266803</td>\n",
              "      <td>0.789434</td>\n",
              "      <td>0.414999</td>\n",
              "      <td>0.507585</td>\n",
              "      <td>...</td>\n",
              "      <td>0.559734</td>\n",
              "      <td>0.510277</td>\n",
              "      <td>0.662607</td>\n",
              "      <td>0.223826</td>\n",
              "      <td>0.614245</td>\n",
              "      <td>0.389197</td>\n",
              "      <td>0.417669</td>\n",
              "      <td>0.314371</td>\n",
              "      <td>0.004807</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.938617</td>\n",
              "      <td>0.776520</td>\n",
              "      <td>0.864251</td>\n",
              "      <td>0.269796</td>\n",
              "      <td>0.762975</td>\n",
              "      <td>0.263984</td>\n",
              "      <td>0.268968</td>\n",
              "      <td>0.782484</td>\n",
              "      <td>0.490950</td>\n",
              "      <td>0.524303</td>\n",
              "      <td>...</td>\n",
              "      <td>0.561327</td>\n",
              "      <td>0.547271</td>\n",
              "      <td>0.663392</td>\n",
              "      <td>0.401270</td>\n",
              "      <td>0.566343</td>\n",
              "      <td>0.507497</td>\n",
              "      <td>0.420561</td>\n",
              "      <td>0.317490</td>\n",
              "      <td>0.002724</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>0.756448</td>\n",
              "      <td>0.873531</td>\n",
              "      <td>0.666991</td>\n",
              "      <td>0.160317</td>\n",
              "      <td>0.729603</td>\n",
              "      <td>0.236810</td>\n",
              "      <td>0.235393</td>\n",
              "      <td>0.863749</td>\n",
              "      <td>0.528729</td>\n",
              "      <td>0.598850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.564920</td>\n",
              "      <td>0.515249</td>\n",
              "      <td>0.680500</td>\n",
              "      <td>0.313600</td>\n",
              "      <td>0.658558</td>\n",
              "      <td>0.466291</td>\n",
              "      <td>0.433929</td>\n",
              "      <td>0.329840</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>0.945845</td>\n",
              "      <td>0.766677</td>\n",
              "      <td>0.872678</td>\n",
              "      <td>0.219189</td>\n",
              "      <td>0.771561</td>\n",
              "      <td>0.273661</td>\n",
              "      <td>0.265504</td>\n",
              "      <td>0.788548</td>\n",
              "      <td>0.482925</td>\n",
              "      <td>0.488530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.564933</td>\n",
              "      <td>0.553154</td>\n",
              "      <td>0.665619</td>\n",
              "      <td>0.245298</td>\n",
              "      <td>0.543855</td>\n",
              "      <td>0.360884</td>\n",
              "      <td>0.417775</td>\n",
              "      <td>0.312038</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>0.990905</td>\n",
              "      <td>0.764080</td>\n",
              "      <td>0.781102</td>\n",
              "      <td>0.227202</td>\n",
              "      <td>0.783425</td>\n",
              "      <td>0.293496</td>\n",
              "      <td>0.263547</td>\n",
              "      <td>0.792985</td>\n",
              "      <td>0.477677</td>\n",
              "      <td>0.498692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.565220</td>\n",
              "      <td>0.537005</td>\n",
              "      <td>0.664877</td>\n",
              "      <td>0.468492</td>\n",
              "      <td>0.592823</td>\n",
              "      <td>0.411176</td>\n",
              "      <td>0.416593</td>\n",
              "      <td>0.312585</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>0.954209</td>\n",
              "      <td>0.772856</td>\n",
              "      <td>0.849587</td>\n",
              "      <td>0.282508</td>\n",
              "      <td>0.763172</td>\n",
              "      <td>0.269291</td>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.792671</td>\n",
              "      <td>0.476287</td>\n",
              "      <td>0.500464</td>\n",
              "      <td>...</td>\n",
              "      <td>0.565755</td>\n",
              "      <td>0.547353</td>\n",
              "      <td>0.663008</td>\n",
              "      <td>0.398836</td>\n",
              "      <td>0.545958</td>\n",
              "      <td>0.514746</td>\n",
              "      <td>0.418520</td>\n",
              "      <td>0.315245</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>0.949232</td>\n",
              "      <td>0.765256</td>\n",
              "      <td>0.849601</td>\n",
              "      <td>0.229488</td>\n",
              "      <td>0.765632</td>\n",
              "      <td>0.256488</td>\n",
              "      <td>0.274963</td>\n",
              "      <td>0.780938</td>\n",
              "      <td>0.479528</td>\n",
              "      <td>0.489782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.565688</td>\n",
              "      <td>0.540031</td>\n",
              "      <td>0.671029</td>\n",
              "      <td>0.383420</td>\n",
              "      <td>0.551319</td>\n",
              "      <td>0.291786</td>\n",
              "      <td>0.416466</td>\n",
              "      <td>0.313401</td>\n",
              "      <td>0.008446</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284807 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b3cf9a7-0d1b-49f5-ae14-ba59aa4d6f06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b3cf9a7-0d1b-49f5-ae14-ba59aa4d6f06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b3cf9a7-0d1b-49f5-ae14-ba59aa4d6f06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove class target from features data\n",
        "y = df[\"class\"].values\n",
        "X = df.drop(columns=\"class\").values"
      ],
      "metadata": {
        "id": "HXEZAyyqqnkv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training/test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "7eEXnoolrVbN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_input_features = len(X_train[0])"
      ],
      "metadata": {
        "id": "w26Im_9etXAv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the deep learning model \n",
        "nn_model = tf.keras.models.Sequential()\n",
        "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\", input_dim=number_input_features))\n",
        "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\"))\n",
        "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
      ],
      "metadata": {
        "id": "zPWnuSp5ra0s"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the model\n",
        "nn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ComUYHsflU",
        "outputId": "d22eb7be-5f4b-4de5-8c8a-2b2aed562bb8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_18 (Dense)            (None, 16)                480       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 769\n",
            "Trainable params: 769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the Sequential model together and customize metrics\n",
        "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "iEZV8bVBsDH5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "fit_model = nn_model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCgZ1jO5sFej",
        "outputId": "6feb2652-e20f-4cb6-945c-43b03150aa7c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0136 - accuracy: 0.9984\n",
            "Epoch 2/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0047 - accuracy: 0.9991\n",
            "Epoch 3/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0040 - accuracy: 0.9992\n",
            "Epoch 4/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0038 - accuracy: 0.9993\n",
            "Epoch 5/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0037 - accuracy: 0.9993\n",
            "Epoch 6/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0037 - accuracy: 0.9993\n",
            "Epoch 7/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0036 - accuracy: 0.9994\n",
            "Epoch 8/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 9/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 10/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 11/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 12/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 13/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0035 - accuracy: 0.9994\n",
            "Epoch 14/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 15/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 16/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 17/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 18/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 19/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 20/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 21/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 22/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 23/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 24/100\n",
            "6676/6676 [==============================] - 13s 2ms/step - loss: 0.0034 - accuracy: 0.9993\n",
            "Epoch 25/100\n",
            "6676/6676 [==============================] - 14s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 26/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 27/100\n",
            "6676/6676 [==============================] - 13s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 28/100\n",
            "6676/6676 [==============================] - 13s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 29/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 30/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 31/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 32/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 33/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 34/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 35/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 36/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 37/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 38/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 39/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 40/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 41/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 42/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 43/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 44/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 45/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 46/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 47/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 48/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 49/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 50/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 51/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 52/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "Epoch 53/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 54/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 55/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 56/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 57/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 58/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 59/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 60/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 61/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 62/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 63/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 64/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 65/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 66/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 67/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 68/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 69/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 70/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 71/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 72/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 73/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9995\n",
            "Epoch 74/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9995\n",
            "Epoch 75/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9995\n",
            "Epoch 76/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 77/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 78/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 79/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 80/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 81/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9995\n",
            "Epoch 82/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 83/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 84/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 85/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 86/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0030 - accuracy: 0.9995\n",
            "Epoch 87/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 88/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 89/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0032 - accuracy: 0.9994\n",
            "Epoch 90/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9995\n",
            "Epoch 91/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 92/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 93/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 94/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 95/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 96/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 97/100\n",
            "6676/6676 [==============================] - 12s 2ms/step - loss: 0.0030 - accuracy: 0.9995\n",
            "Epoch 98/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 99/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 100/100\n",
            "6676/6676 [==============================] - 11s 2ms/step - loss: 0.0030 - accuracy: 0.9994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn_model.evaluate(X_test,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClQZm6G0sABe",
        "outputId": "5e0b2be0-08ac-4b6c-a085-b1de08eae1f2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2226/2226 - 2s - loss: 0.0029 - accuracy: 0.9994 - 2s/epoch - 1ms/step\n",
            "Loss: 0.002858184278011322, Accuracy: 0.9993960857391357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a method that creates a new Sequential model with hyperparameter options\n",
        "def create_model(hp):\n",
        "    nn_model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
        "    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n",
        "    \n",
        "    # Allow kerastuner to decide number of neurons in first layer\n",
        "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
        "        min_value=1,\n",
        "        max_value=25,\n",
        "        step=2), activation=activation, input_dim=number_input_features))\n",
        "\n",
        "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
        "    for i in range(hp.Int('num_layers', 1, 6)):\n",
        "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "            min_value=1,\n",
        "            max_value=25,\n",
        "            step=2),\n",
        "            activation=activation))\n",
        "    \n",
        "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model\n",
        "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return nn_model"
      ],
      "metadata": {
        "id": "Tr7r5jH-zxkx"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLGwZMxg3Ym1",
        "outputId": "1d2510c2-3744-44bf-81d9-da969d2756e5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.2.1-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.6/169.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.25.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.30.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (23.1.21)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.51.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (4.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.38.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Installing collected packages: kt-legacy, jedi, keras-tuner\n",
            "Successfully installed jedi-0.18.2 keras-tuner-1.2.1 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the kerastuner library\n",
        "import keras_tuner as kt\n",
        "tuner = kt.Hyperband(\n",
        "    create_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=50,\n",
        "    hyperband_iterations=2)"
      ],
      "metadata": {
        "id": "ZbiWhHzb12U5"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the kerastuner search for best hyperparameters\n",
        "tuner.search(X_train,y_train,epochs=20,validation_data=(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rOuAZLbR2l3V",
        "outputId": "f3eb3874-c212-429f-d129-9c93c40a2f57"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "relu              |tanh              |activation\n",
            "13                |15                |first_units\n",
            "2                 |1                 |num_layers\n",
            "1                 |1                 |units_0\n",
            "13                |3                 |units_1\n",
            "5                 |9                 |units_2\n",
            "17                |21                |units_3\n",
            "9                 |17                |units_4\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "3                 |3                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            "6676/6676 [==============================] - 18s 3ms/step - loss: 0.0303 - accuracy: 0.9948 - val_loss: 0.0066 - val_accuracy: 0.9984\n",
            "Epoch 2/2\n",
            "6676/6676 [==============================] - 18s 3ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0036 - val_accuracy: 0.9992\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-1be0b355b760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the kerastuner search for best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_trial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# Display needs the updated trial scored by the Oracle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    383\u001b[0m                     \u001b[0;34m\"Number of consecutive failures excceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\", line 250, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\", line 215, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/tuners/hyperband.py\", line 404, in run_trial\n    return super().run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\", line 286, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/hypermodel.py\", line 144, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 1848, in _check_data_cardinality\n    raise ValueError(msg)\nValueError: Data cardinality is ambiguous:\n  x sizes: 57579\n  y sizes: 213605\nMake sure all arrays contain the same number of samples.\n"
          ]
        }
      ]
    }
  ]
}